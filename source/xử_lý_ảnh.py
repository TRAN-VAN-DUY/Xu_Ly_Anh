# -*- coding: utf-8 -*-
"""X·ª≠ L√Ω ·∫£nh

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KfwwVnJVYUHBlt_zozpM-nangMQkq0lw
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/facebookresearch/sam2.git
# %cd sam2
!pip install -e ".[notebooks]"

import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sam2.sam2_image_predictor import SAM2ImagePredictor
from sam2.build_sam import build_sam2

#Load checkpoint t·ª´ Hugging Face
predictor = SAM2ImagePredictor.from_pretrained("facebook/sam2-hiera-large")

#Ki·ªÉm tra thi·∫øt b·ªã
device = "cuda" if torch.cuda.is_available() else "cpu"
print (f"Using device: {device}")

from google.colab import files

uploaded = files.upload()  # upload ·∫£nh t·ª´ m√°y
image_path = list(uploaded.keys())[0]

image = cv2.imread(image_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

plt.imshow(image)
plt.title("Input Image")
plt.axis("off")
plt.show()

#prompt
# V√≠ d·ª•: 2 ƒëi·ªÉm (foreground + background)
point_coords = torch.tensor([[300, 400], [100, 150]], dtype=torch.float32)
point_labels = torch.tensor([1, 0], dtype=torch.int64)  # 1=foreground, 0=background

with torch.inference_mode(), torch.autocast(device_type=device if device=="cuda" else "cpu", dtype=torch.bfloat16 if device=="cuda" else torch.float32):
    predictor.set_image(image)
    masks, scores, logits = predictor.predict(
        point_coords=point_coords,
        point_labels=point_labels,
        multimask_output=True
    )

#Hi·ªÉn th·ªã mask
# Ch·ªçn mask t·ªët nh·∫•t theo score
best_mask = masks[np.argmax(scores)]

plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.title("Input Image")
plt.imshow(image)
plt.axis("off")

plt.subplot(1,2,2)
plt.title("SAM 2 Mask")
plt.imshow(best_mask, cmap="gray")
plt.axis("off")
plt.show()

background_color = [0, 0 , 0]
background = np.zeros_like(image)
background[:] = background_color

# Gh√©p foreground l√™n background
composite = background.copy()
composite[best_mask == 1] = image[best_mask == 1]

plt.imshow(composite)
plt.title("Foreground on Green Background")
plt.axis("off")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Clone repo
!git clone https://github.com/facebookresearch/segment-anything.git
# %cd segment-anything

# C√†i PyTorch GPU (Colab th∆∞·ªùng ƒë√£ c√≥ s·∫µn)
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# C√†i c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
!pip install opencv-python matplotlib scikit-image pandas
!pip install git+https://github.com/facebookresearch/segment-anything.git

!mkdir checkpoints
!wget -O checkpoints/sam_vit_h.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

import gradio as gr
from segment_anything import sam_model_registry, SamPredictor
import torch
import cv2
import numpy as np

# Load model
sam_checkpoint = "checkpoints/sam_vit_h.pth"
model_type = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)
predictor = SamPredictor(sam)

# H√†m inference cho Gradio
def segment_image(image, x, y):
    image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)
    predictor.set_image(image)
    input_point = np.array([[x, y]])
    input_label = np.array([1])
    masks, scores, _ = predictor.predict(point_coords=input_point,
                                         point_labels=input_label,
                                         multimask_output=True)
    mask = masks[0]
    mask_img = (mask*255).astype(np.uint8)
    return mask_img

# Ch·∫°y Gradio interface
iface = gr.Interface(
    fn=segment_image,
    inputs=[gr.Image(type="numpy"), gr.Number(label="X"), gr.Number(label="Y")],
    outputs=gr.Image(type="numpy")
)

iface.launch(share=True)  # share=True ƒë·ªÉ t·∫°o link public Colab

import gradio as gr
from segment_anything import sam_model_registry, SamPredictor
import torch
import cv2
import numpy as np

# Load model SAM
sam_checkpoint = "checkpoints/sam_vit_h.pth"
model_type = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)
predictor = SamPredictor(sam)


# ==========================
# 1) Segment b·∫±ng Point
# ==========================
def segment_by_point(image, x, y):
    image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)
    predictor.set_image(image)

    input_point = np.array([[x, y]])
    input_label = np.array([1])      # 1 = positive point

    masks, scores, _ = predictor.predict(
        point_coords=input_point,
        point_labels=input_label,
        multimask_output=True
    )
    mask = masks[0]
    return (mask * 255).astype(np.uint8)


# ==========================
# 2) Segment b·∫±ng Bounding Box
# ==========================
def segment_by_box(image, x1, y1, x2, y2):
    image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)
    predictor.set_image(image)

    box = np.array([x1, y1, x2, y2])

    masks, scores, _ = predictor.predict(
        box=box[None, :],    # shape (1,4)
        multimask_output=True
    )
    mask = masks[0]
    return (mask * 255).astype(np.uint8)


# ==========================
# 3) Auto mask (kh√¥ng c·∫ßn prompt)
# ==========================
def auto_mask(image):
    image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)
    predictor.set_image(image)

    # L·∫•y embedding ‚Üí auto-mask
    masks, scores, _ = predictor.predict(
        multimask_output=True
    )

    mask = masks[0]
    return (mask * 255).astype(np.uint8)


# ==========================
# Build UI
# ==========================

with gr.Blocks() as demo:

    gr.Markdown("## üü¶ Segment Anything ‚Äì Multi Interface")

    with gr.Tabs():

        # Tab 1 ‚Äî point
        with gr.Tab("üîµ Point Prompt"):
            input_img_1 = gr.Image(type="numpy")
            x_in = gr.Number(label="X")
            y_in = gr.Number(label="Y")
            out_1 = gr.Image(type="numpy")
            btn1 = gr.Button("Segment")
            btn1.click(segment_by_point, inputs=[input_img_1, x_in, y_in], outputs=out_1)

        # Tab 2 ‚Äî box
        with gr.Tab("üü© Box Prompt"):
            input_img_2 = gr.Image(type="numpy")
            x1 = gr.Number(label="x1")
            y1 = gr.Number(label="y1")
            x2 = gr.Number(label="x2")
            y2 = gr.Number(label="y2")
            out_2 = gr.Image(type="numpy")
            btn2 = gr.Button("Segment")
            btn2.click(segment_by_box, inputs=[input_img_2, x1, y1, x2, y2], outputs=out_2)

        # Tab 3 ‚Äî auto-mask
        with gr.Tab("‚öô Auto Mask"):
            input_img_3 = gr.Image(type="numpy")
            out_3 = gr.Image(type="numpy")
            btn3 = gr.Button("Run")
            btn3.click(auto_mask, inputs=input_img_3, outputs=out_3)

demo.launch(share=True)

import gradio as gr
from segment_anything import sam_model_registry, SamPredictor
import torch
import cv2
import numpy as np

# Load SAM model
sam_checkpoint = "checkpoints/sam_vit_h.pth"
model_type = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device)
predictor = SamPredictor(sam)


def click_and_segment(image, points, labels, point_type, evt: gr.SelectData):
    if image is None:
        return None, points, labels

    x, y = evt.index

    # 1 = POS ; 0 = NEG
    points.append([x, y])
    labels.append(point_type)

    img = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)
    predictor.set_image(img)

    pts = np.array(points)
    lbls = np.array(labels)

    masks, _, _ = predictor.predict(
        point_coords=pts,
        point_labels=lbls,
        multimask_output=False
    )

    mask = (masks[0] * 255).astype(np.uint8)
    return mask, points, labels


with gr.Blocks() as demo:
    gr.Markdown("## üü¶ SAM Interactive Segmentation (POS / NEG)")

    points_state = gr.State([])
    labels_state = gr.State([])

    point_type = gr.Radio(
        label="Lo·∫°i ƒëi·ªÉm",
        choices=[("Positive", 1), ("Negative", 0)],
        value=("Positive", 1),
        type="value"
    )

    with gr.Row():
        input_img = gr.Image(type="numpy", label="Input Image")
        output_mask = gr.Image(type="numpy", label="Mask Output")

    # Click event
    input_img.select(
        fn=click_and_segment,
        inputs=[input_img, points_state, labels_state, point_type],
        outputs=[output_mask, points_state, labels_state],
    )

demo.launch(share=True)

import gradio as gr
from segment_anything import sam_model_registry, SamPredictor
import torch
import cv2
import numpy as np

# Load SAM model
sam_checkpoint = "checkpoints/sam_vit_h.pth"
model_type = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device)
predictor = SamPredictor(sam)


def click_and_segment(image, points, labels, point_type, evt: gr.SelectData):
    """
    Th·ª±c hi·ªán ph√¢n ƒëo·∫°n SAM d·ª±a tr√™n ƒëi·ªÉm click, gi·ªØ l·∫°i v√πng ƒë∆∞·ª£c segment v√† l√†m ƒëen ph·∫ßn c√≤n l·∫°i.
    """
    if image is None:
        return None, points, labels

    x, y = evt.index

    # C·∫≠p nh·∫≠t danh s√°ch ƒëi·ªÉm click v√† nh√£n (1: Positive, 0: Negative)
    points.append([x, y])
    labels.append(point_type)

    # Gradio Image (type="numpy") l√† RGB. cv2 v√† SAM predictor c·∫ßn BGR.
    img_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    predictor.set_image(img_bgr)

    pts = np.array(points)
    lbls = np.array(labels)

    # D·ª± ƒëo√°n mask
    masks, _, _ = predictor.predict(
        point_coords=pts,
        point_labels=lbls,
        multimask_output=False
    )

    mask = masks[0] # Mask boolean (True/False)

    # -----------------------------------------------------
    # Logic: Gi·ªØ Foreground (V√πng mask), ƒêen Background
    # -----------------------------------------------------

    # 1. T·∫°o m·ªôt ·∫£nh ƒëen ho√†n to√†n c√≥ c√πng k√≠ch th∆∞·ªõc v·ªõi ·∫£nh g·ªëc.
    extracted_image = np.zeros_like(img_bgr, dtype=np.uint8)

    # 2. G√°n c√°c pixel c·ªßa ·∫£nh g·ªëc v√†o v·ªã tr√≠ t∆∞∆°ng ·ª©ng trong ·∫£nh ƒëen
    # n∆°i mask l√† True.
    extracted_image[mask] = img_bgr[mask]

    # Chuy·ªÉn ƒë·ªïi t·ª´ BGR sang RGB ƒë·ªÉ hi·ªÉn th·ªã ƒë√∫ng trong Gradio
    extracted_image_rgb = cv2.cvtColor(extracted_image, cv2.COLOR_BGR2RGB)

    # -----------------------------------------------------

    # 3. (T√πy ch·ªçn) V·∫Ω c√°c ƒëi·ªÉm ƒë√£ click l√™n ·∫£nh ƒë·ªÉ ng∆∞·ªùi d√πng d·ªÖ h√¨nh dung
    # Ta v·∫Ω l√™n ·∫£nh RGB ƒë√£ ƒë∆∞·ª£c t√°ch n·ªÅn
    for p, l in zip(points, labels):
        color = (0, 255, 0) if l == 1 else (255, 0, 0) # Xanh l√° cho POS, ƒê·ªè cho NEG
        cv2.circle(extracted_image_rgb, (p[0], p[1]), radius=5, color=color, thickness=-1)

    # Tr·∫£ v·ªÅ ·∫£nh ƒë√£ ƒë∆∞·ª£c t√°ch n·ªÅn
    return extracted_image_rgb, points, labels


# -----------------------------------------------------
# Giao di·ªán Gradio (gi·ªØ nguy√™n)
# -----------------------------------------------------

with gr.Blocks() as demo:
    gr.Markdown("## üü¶ SAM Interactive Segmentation (POS / NEG) - Foreground Extraction")

    points_state = gr.State([]) # L∆∞u t·ªça ƒë·ªô ƒëi·ªÉm
    labels_state = gr.State([]) # L∆∞u nh√£n ƒëi·ªÉm (1 ho·∫∑c 0)

    with gr.Row():
        gr.Markdown("### 1. Ch·ªçn Lo·∫°i ƒêi·ªÉm")
        gr.Markdown("### 2. T·∫£i ·∫¢nh v√† Click")

    point_type = gr.Radio(
        label="Lo·∫°i ƒëi·ªÉm",
        choices=[("Positive (ƒê·ªëi t∆∞·ª£ng)", 1), ("Negative (N·ªÅn/V√πng Lo·∫°i b·ªè)", 0)],
        value=1, # Gi√° tr·ªã m·∫∑c ƒë·ªãnh l√† 1 (Positive)
        type="value"
    )

    with gr.Row():
        input_img = gr.Image(type="numpy", label="Input Image (Click ƒë·ªÉ Segment)")
        output_image = gr.Image(type="numpy", label="·∫¢nh ƒê√£ T√°ch N·ªÅn (Foreground Only)")

    with gr.Row():
        reset_btn = gr.Button("Reset ƒêi·ªÉm & ·∫¢nh")

        def reset_all():
            return None, [], []

        reset_btn.click(
            fn=reset_all,
            inputs=[],
            outputs=[output_image, points_state, labels_state]
        )

    # Click event
    input_img.select(
        fn=click_and_segment,
        inputs=[input_img, points_state, labels_state, point_type],
        outputs=[output_image, points_state, labels_state],
    )

demo.launch(share=True)

