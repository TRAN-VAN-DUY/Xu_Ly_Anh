# -*- coding: utf-8 -*-
"""X·ª≠ L√Ω ·∫£nh

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KfwwVnJVYUHBlt_zozpM-nangMQkq0lw
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/facebookresearch/sam2.git
# %cd sam2
!pip install -e ".[notebooks]"

import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sam2.sam2_image_predictor import SAM2ImagePredictor
from sam2.build_sam import build_sam2

#Load checkpoint t·ª´ Hugging Face
predictor = SAM2ImagePredictor.from_pretrained("facebook/sam2-hiera-large")

#Ki·ªÉm tra thi·∫øt b·ªã
device = "cuda" if torch.cuda.is_available() else "cpu"
print (f"Using device: {device}")

from google.colab import files

uploaded = files.upload()  # upload ·∫£nh t·ª´ m√°y
image_path = list(uploaded.keys())[0]

image = cv2.imread(image_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

plt.imshow(image)
plt.title("Input Image")
plt.axis("off")
plt.show()

#prompt
# V√≠ d·ª•: 2 ƒëi·ªÉm (foreground + background)
point_coords = torch.tensor([[300, 400], [100, 150]], dtype=torch.float32)
point_labels = torch.tensor([1, 0], dtype=torch.int64)  # 1=foreground, 0=background

with torch.inference_mode(), torch.autocast(device_type=device if device=="cuda" else "cpu", dtype=torch.bfloat16 if device=="cuda" else torch.float32):
    predictor.set_image(image)
    masks, scores, logits = predictor.predict(
        point_coords=point_coords,
        point_labels=point_labels,
        multimask_output=True
    )

#Hi·ªÉn th·ªã mask
# Ch·ªçn mask t·ªët nh·∫•t theo score
best_mask = masks[np.argmax(scores)]

plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.title("Input Image")
plt.imshow(image)
plt.axis("off")

plt.subplot(1,2,2)
plt.title("SAM 2 Mask")
plt.imshow(best_mask, cmap="gray")
plt.axis("off")
plt.show()

background_color = [0, 0 , 0]
background = np.zeros_like(image)
background[:] = background_color

# Gh√©p foreground l√™n background
composite = background.copy()
composite[best_mask == 1] = image[best_mask == 1]

plt.imshow(composite)
plt.title("Foreground on Green Background")
plt.axis("off")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# Clone repo
!git clone https://github.com/facebookresearch/segment-anything.git
# %cd segment-anything

# C√†i PyTorch GPU (Colab th∆∞·ªùng ƒë√£ c√≥ s·∫µn)
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# C√†i c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
!pip install opencv-python matplotlib scikit-image pandas
!pip install git+https://github.com/facebookresearch/segment-anything.git

!mkdir checkpoints
!wget -O checkpoints/sam_vit_h.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

import gradio as gr
from segment_anything import sam_model_registry, SamPredictor
import torch
import cv2
import numpy as np

# Load model
sam_checkpoint = "checkpoints/sam_vit_h.pth"
model_type = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)
predictor = SamPredictor(sam)

# H√†m inference cho Gradio
def segment_image(image, x, y):
    image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)
    predictor.set_image(image)
    input_point = np.array([[x, y]])
    input_label = np.array([1])
    masks, scores, _ = predictor.predict(point_coords=input_point,
                                         point_labels=input_label,
                                         multimask_output=True)
    mask = masks[0]
    mask_img = (mask*255).astype(np.uint8)
    return mask_img

# Ch·∫°y Gradio interface
iface = gr.Interface(
    fn=segment_image,
    inputs=[gr.Image(type="numpy"), gr.Number(label="X"), gr.Number(label="Y")],
    outputs=gr.Image(type="numpy")
)

iface.launch(share=True)  # share=True ƒë·ªÉ t·∫°o link public Colab

import gradio as gr
from segment_anything import sam_model_registry, SamPredictor
import torch
import cv2
import numpy as np

# Load model SAM
sam_checkpoint = "checkpoints/sam_vit_h.pth"
model_type = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)
predictor = SamPredictor(sam)


# ==========================
# 1) Segment b·∫±ng Point
# ==========================
def segment_by_point(image, x, y):
    image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)
    predictor.set_image(image)

    input_point = np.array([[x, y]])
    input_label = np.array([1])      # 1 = positive point

    masks, scores, _ = predictor.predict(
        point_coords=input_point,
        point_labels=input_label,
        multimask_output=True
    )
    mask = masks[0]
    return (mask * 255).astype(np.uint8)


# ==========================
# 2) Segment b·∫±ng Bounding Box
# ==========================
def segment_by_box(image, x1, y1, x2, y2):
    image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)
    predictor.set_image(image)

    box = np.array([x1, y1, x2, y2])

    masks, scores, _ = predictor.predict(
        box=box[None, :],    # shape (1,4)
        multimask_output=True
    )
    mask = masks[0]
    return (mask * 255).astype(np.uint8)


# ==========================
# 3) Auto mask (kh√¥ng c·∫ßn prompt)
# ==========================
def auto_mask(image):
    image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)
    predictor.set_image(image)

    # L·∫•y embedding ‚Üí auto-mask
    masks, scores, _ = predictor.predict(
        multimask_output=True
    )

    mask = masks[0]
    return (mask * 255).astype(np.uint8)


# ==========================
# Build UI
# ==========================

with gr.Blocks() as demo:

    gr.Markdown("## üü¶ Segment Anything ‚Äì Multi Interface")

    with gr.Tabs():

        # Tab 1 ‚Äî point
        with gr.Tab("üîµ Point Prompt"):
            input_img_1 = gr.Image(type="numpy")
            x_in = gr.Number(label="X")
            y_in = gr.Number(label="Y")
            out_1 = gr.Image(type="numpy")
            btn1 = gr.Button("Segment")
            btn1.click(segment_by_point, inputs=[input_img_1, x_in, y_in], outputs=out_1)

        # Tab 2 ‚Äî box
        with gr.Tab("üü© Box Prompt"):
            input_img_2 = gr.Image(type="numpy")
            x1 = gr.Number(label="x1")
            y1 = gr.Number(label="y1")
            x2 = gr.Number(label="x2")
            y2 = gr.Number(label="y2")
            out_2 = gr.Image(type="numpy")
            btn2 = gr.Button("Segment")
            btn2.click(segment_by_box, inputs=[input_img_2, x1, y1, x2, y2], outputs=out_2)

        # Tab 3 ‚Äî auto-mask
        with gr.Tab("‚öô Auto Mask"):
            input_img_3 = gr.Image(type="numpy")
            out_3 = gr.Image(type="numpy")
            btn3 = gr.Button("Run")
            btn3.click(auto_mask, inputs=input_img_3, outputs=out_3)

demo.launch(share=True)

import gradio as gr
from segment_anything import sam_model_registry, SamPredictor
import torch
import cv2
import numpy as np

# Load SAM model
sam_checkpoint = "checkpoints/sam_vit_h.pth"
model_type = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device)
predictor = SamPredictor(sam)


# ==========================
# Multi-point segmentation
# ==========================
def segment_multi_points(image, points_text):
    """
    points_text d·∫°ng:
        120,200
        150,180
        100,250
    """

    if points_text.strip() == "":
        return None

    # Parse danh s√°ch ƒëi·ªÉm
    pts = []
    for line in points_text.split("\n"):
        line = line.strip()
        if line == "":
            continue
        try:
            x, y = map(float, line.split(","))
            pts.append([x, y])
        except:
            return "Sai format! Ph·∫£i l√†: x,y"

    pts = np.array(pts)
    labels = np.ones(len(pts))  # T·∫•t c·∫£ ƒë·ªÅu POSITIVE

    # Run SAM
    image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)
    predictor.set_image(image)

    masks, scores, _ = predictor.predict(
        point_coords=pts,
        point_labels=labels,
        multimask_output=True
    )

    mask = masks[0]
    return (mask * 255).astype(np.uint8)


# ==========================
# Build UI
# ==========================

with gr.Blocks() as demo:

    gr.Markdown("## üü¶ SAM Multi-Point Prompt Segmentation")

    with gr.Row():
        input_img = gr.Image(type="numpy", label="Input Image")
        output_mask = gr.Image(type="numpy", label="Mask Output")

    points_box = gr.Textbox(
        label="Nh·∫≠p nhi·ªÅu point (x,y m·ªói d√≤ng)",
        placeholder="V√≠ d·ª•:\n120,200\n150,180\n100,250"
    )

    run_btn = gr.Button("Ch·∫°y Multi-Point Segmentation")

    run_btn.click(segment_multi_points, inputs=[input_img, points_box], outputs=output_mask)


demo.launch(share=True)